# Session 07 — Cross-Source Validation & Release: Implementation Plan

## Mission

Validate the full Cold Start Watchtower RAG pipeline across local folders and Google Drive, update documentation to reflect the provider model, and produce a go/no-go release recommendation.

---

## Phase 1: Run Quality Gates & Fix Regressions

### Step 1.1 — Run Rust quality gates

Run the mandatory CI checklist on the current branch state:

```bash
cargo fmt --all && cargo fmt --all --check
RUSTFLAGS="-D warnings" cargo test --workspace
cargo clippy --workspace -- -D warnings
```

**Expected outcome:** All gates pass (Session 06 left them green: 1664 tests, 0 warnings, 0 clippy).

**If failures occur:** Fix them in priority order:
1. Compilation errors (type mismatches, missing imports)
2. Test failures (broken assertions, outdated mock structs)
3. Clippy warnings (unused code, redundant clones)
4. Formatting issues (run `cargo fmt --all` again)

### Step 1.2 — Run dashboard quality gates

```bash
cd dashboard && npm run check
```

**Expected outcome:** 0 errors, 5 pre-existing warnings (all from non-Watchtower code).

**If new warnings appear:** Fix any warnings originating from files modified in Sessions 05-06:
- `ContentSourcesSection.svelte`
- `SourcesStep.svelte`
- `onboarding.ts`
- `api.ts`
- `+page.svelte` (onboarding)

### Step 1.3 — Verify existing test coverage

Audit that the critical paths have test coverage:

| Path | Test Location | Expected Tests |
|------|---------------|----------------|
| `ingest_content()` (shared pipeline) | `source/tests.rs` | `ingest_parity_local_vs_direct_content`, `ingest_content_dedup_by_hash` |
| `LocalFsProvider` scan/read | `source/tests.rs` | 5 tests (scan, read, error, filter, hidden) |
| `GoogleDriveProvider` ID extraction | `source/tests.rs` | 2 tests (format, raw) |
| Storage helpers (ensure/find) | `source/tests.rs` | 3 tests (create_once, folder_id, coexist) |
| Config round-trip (Drive fields) | `config/tests.rs` | 3 tests (drive, mixed, json_patch) |
| Front-matter parsing | `watchtower/tests.rs` | Multiple tests (yaml extraction, tags, malformed) |
| Pattern matching | `watchtower/tests.rs` | Multiple tests (md, txt, nested) |
| Seed worker | `seed_worker.rs` | 5 tests (parse, mock LLM, etc.) |
| Winning DNA | `context/winning_dna.rs` | 15+ tests (classify, score, retrieve, format) |
| Watchtower loop | `watchtower/tests.rs` | Cancellation, cooldown, directory walk tests |

**Decision:** If coverage gaps exist for cross-provider scenarios, add targeted tests (see Phase 2).

---

## Phase 2: End-to-End Shakeout

This phase writes integration tests and performs manual verification of the complete workflow. **No new business logic** — only tests and minor fixes.

### Step 2.1 — Write E2E integration test: Local folder ingest → seed creation

**File to create:** `crates/tuitbot-core/src/source/integration_tests.rs` (or add to existing `source/tests.rs` if it stays under 500 lines)

**Test: `e2e_local_folder_ingest_to_seed_pipeline`**

1. Create a tempdir with 2 `.md` files (one with YAML front-matter, one plain)
2. Call `ingest_file()` for each file
3. Verify `content_nodes` are created with correct status='pending'
4. Verify front-matter was parsed (title, tags)
5. Verify content hash dedup (re-ingest same content → Skipped)
6. Verify `get_pending_content_nodes()` returns the nodes
7. Run `SeedWorker.process_node()` with a mock LLM
8. Verify `draft_seeds` table has seeds
9. Verify `get_seeds_for_context()` returns seeds
10. Call `build_draft_context()` and verify it returns the cold-start seeds path (no ancestors exist)

**Rationale:** This is the critical happy path that validates the full pipeline works end-to-end with local files. Session 06 tested individual components but not the full chain.

### Step 2.2 — Write E2E integration test: Simulated Google Drive ingest

**Test: `e2e_google_drive_ingest_to_seed_pipeline`**

1. Register a `google_drive` source via `ensure_google_drive_source()`
2. Call `ingest_content()` directly with `gdrive://fileA/notes.md` provider ID and markdown content
3. Verify content node is created with the `gdrive://` relative_path
4. Call `ingest_content()` again with same content → verify `Skipped`
5. Call `ingest_content()` with updated content → verify `Updated`
6. Verify the node's status resets to 'pending' after update
7. Run seed generation on the node
8. Verify `build_draft_context()` returns the seeds

**Rationale:** Validates the Google Drive code path through `ingest_content()` without requiring a real Google API. The provider's `scan_for_changes()` and `read_content()` methods are already tested in isolation.

### Step 2.3 — Write E2E integration test: Manual POST /api/ingest with inline nodes

**File:** Add test to `crates/tuitbot-server/src/routes/ingest.rs` (or a new server integration test if appropriate)

**Test: `ingest_inline_nodes_creates_manual_source`**

1. Start an in-memory test server with `init_test_db()`
2. POST to `/api/ingest` with `inline_nodes` containing 2 nodes
3. Verify response has `ingested: 2, skipped: 0`
4. POST again with same content → verify `skipped: 2`
5. Verify `source_contexts` table has a "manual" source
6. Verify `content_nodes` table has 2 nodes linked to the manual source

**Rationale:** Validates the API → storage path for Shortcuts/Telegram ingestion, which is a distinct entry point from file watching.

### Step 2.4 — Write integration test: Mixed sources coexist and feed Winning DNA

**Test: `mixed_sources_feed_draft_context`**

1. Create both a `local_fs` and a `google_drive` source
2. Ingest content through both
3. Generate seeds from both sources
4. Call `build_draft_context()` and verify seeds from both sources appear
5. Verify `get_seeds_for_context()` returns seeds ordered by engagement_weight

**Rationale:** Validates that the provider model correctly isolates sources while the Winning DNA pipeline aggregates across all of them.

### Step 2.5 — Write integration test: Loopback metadata write-back

**Test: `loopback_writes_metadata_to_source_file`**

1. Create a tempdir with a `.md` file containing YAML front-matter
2. Ingest the file
3. Call `write_metadata_to_file()` with a mock tweet entry
4. Read the file back and verify:
   - Original front-matter is preserved
   - `tuitbot` array is appended with the tweet metadata
5. Call `write_metadata_to_file()` again with same `tweet_id` → verify idempotent (no duplicate)
6. Re-ingest the file → verify content hash changed (front-matter was updated)

**Rationale:** Validates that the loopback cycle (ingest → post → write-back → re-ingest) doesn't corrupt user files or cause infinite loops.

### Step 2.6 — Tauri folder-pick flow verification (documented manual test)

This cannot be automated; document the test procedure for the validation report:

**Manual test procedure:**
1. Launch `cd dashboard && npm run tauri dev`
2. Navigate to Onboarding → Sources step
3. Click "Browse" — verify native file dialog opens
4. Select a folder containing `.md` files
5. Verify the path appears in the input field
6. Complete onboarding
7. Verify `config.toml` contains `[[content_sources.sources]]` with the selected path
8. Navigate to Settings → Content Sources
9. Verify the source is displayed with correct path and patterns
10. Change source type to "Google Drive"
11. Verify Drive-specific fields appear (folder_id, service_account_key)
12. Switch back to "Local Folder"
13. Verify local-specific fields reappear

**Record result as:** PASS / FAIL with any notes for the validation report.

### Step 2.7 — Simulated Google Drive sync verification (documented manual test)

Since we can't test against real Google APIs without credentials, document:

**Manual test procedure (API simulation):**
1. Start `cargo run -p tuitbot-server`
2. POST `/api/ingest` with inline nodes simulating Drive content:
   ```json
   {
     "inline_nodes": [
       {
         "relative_path": "gdrive://abc123/my-notes.md",
         "body_text": "---\ntitle: Product Strategy\ntags: [strategy, growth]\n---\nOur key differentiator is...",
         "title": "Product Strategy"
       }
     ]
   }
   ```
3. Verify response: `ingested: 1`
4. Query the DB to verify the content_node exists with the `gdrive://` path

**Rationale:** While we can't test real Google Drive auth (requires service account key), we can verify the ingest pipeline handles Drive-format provider IDs correctly through the API.

---

## Phase 3: Update Documentation

### Step 3.1 — Update `docs/architecture.md`

**File:** `docs/architecture.md`

**Changes to make:**

1. Add a new section **"Content Source Pipeline"** between "Runtime Loops" and "AI Assist Endpoints":

   ```markdown
   ## Content Source Pipeline (Watchtower)

   The Watchtower subsystem ingests content from external sources, extracts
   draft seeds, and enriches the content generation pipeline via Winning DNA
   retrieval.

   ### Provider Model

   Content sources implement the `ContentSourceProvider` trait (`core::source/`):

   | Provider | Module | Mechanism | Status |
   |----------|--------|-----------|--------|
   | `local_fs` | `source/local_fs.rs` | `notify` watcher + fallback polling | Stable |
   | `google_drive` | `source/google_drive.rs` | Interval polling via Drive API v3 | Stable (read-only) |
   | `manual` | (inline via API) | Direct `POST /api/ingest` | Stable |

   ### Pipeline Flow

   ```
   Source → scan/watch → ingest_content() → content_nodes → SeedWorker → draft_seeds → Winning DNA → draft pipeline
   ```

   ### Storage Tables

   | Table | Purpose |
   |-------|---------|
   | `source_contexts` | Registered sources with sync state |
   | `content_nodes` | Ingested content with dedup by (source_id, relative_path, hash) |
   | `draft_seeds` | Pre-computed hooks/angles for cold-start context |
   ```

2. Update the **Key Modules** table to add:

   | Module | Notes |
   |--------|-------|
   | `core/source/` | `ContentSourceProvider` trait; `LocalFsProvider`, `GoogleDriveProvider` implementations |
   | `core/automation/watchtower/` | File watcher, remote polling, shared ingest pipeline, loop-back metadata |
   | `core/automation/seed_worker.rs` | Background LLM worker extracting draft seeds from content nodes |
   | `core/context/winning_dna.rs` | Archetype classification, engagement scoring, ancestor retrieval, cold-start seeds |

3. Update the **Runtime Loops** table to add:

   | Loop | Autopilot | Composer |
   |---|---|---|
   | Watchtower (content sources) | Active | Active |
   | Seed worker | Active | Active |

### Step 3.2 — Update `docs/configuration.md`

**File:** `docs/configuration.md`

**Changes to make:**

1. Add `[content_sources]` to the **Config Sections** table:

   | Section | Purpose |
   |---------|---------|
   | `[content_sources]` | Content source configuration (local folders, Google Drive) |

2. Add a new section **"Content Sources"** after the "Validation" section:

   ```markdown
   ## Content Sources

   Configure external content sources for the Watchtower ingest pipeline.
   Content is ingested as notes, processed into draft seeds, and used to
   enrich AI-generated content via Winning DNA retrieval.

   ### Local Folder Source

   ```toml
   [[content_sources.sources]]
   source_type = "local_fs"
   path = "~/Obsidian/my-vault"
   watch = true
   file_patterns = ["*.md", "*.txt"]
   loop_back_enabled = true
   ```

   | Field | Default | Description |
   |-------|---------|-------------|
   | `source_type` | `"local_fs"` | Source type identifier |
   | `path` | — | Path to content directory (supports `~` expansion) |
   | `watch` | `true` | Watch for real-time file changes |
   | `file_patterns` | `["*.md", "*.txt"]` | Glob patterns for files to ingest |
   | `loop_back_enabled` | `true` | Write tweet metadata back to source file front-matter |

   ### Google Drive Source

   ```toml
   [[content_sources.sources]]
   source_type = "google_drive"
   folder_id = "1abc..."
   service_account_key = "~/.tuitbot/service-account.json"
   watch = true
   file_patterns = ["*.md", "*.txt"]
   poll_interval_seconds = 300
   loop_back_enabled = false
   ```

   | Field | Default | Description |
   |-------|---------|-------------|
   | `source_type` | — | Must be `"google_drive"` |
   | `folder_id` | — | Google Drive folder ID to monitor |
   | `service_account_key` | — | Path to Google service account JSON key file |
   | `poll_interval_seconds` | `300` | Seconds between Drive API polls |
   | `loop_back_enabled` | `false` | Not supported for Drive (read-only) |

   ### Operational Limits

   | Parameter | Value | Notes |
   |-----------|-------|-------|
   | Max file size | Unbounded (content truncated at 2000 chars for seed extraction) | Full content stored in DB |
   | File types | `.md`, `.txt` only | Configurable via `file_patterns` |
   | Dedup | SHA-256 content hash per (source, path) | Unchanged content is skipped |
   | Seed generation | 5 nodes per batch, every 5 minutes | Low-priority background worker |
   | RAG context | Max 5 ancestors or 5 cold-start seeds, 2000 chars | Injected into LLM prompts |

   ### Manual Ingest API

   Content can also be submitted directly via the HTTP API:

   ```bash
   curl -X POST http://localhost:3001/api/ingest \
     -H "Authorization: Bearer $TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
       "inline_nodes": [{
         "relative_path": "idea.md",
         "body_text": "# My Idea\nContent here...",
         "title": "My Idea"
       }]
     }'
   ```
   ```

### Step 3.3 — Verify documentation accuracy

After writing the doc updates, cross-reference:
- Config field names match `types.rs` exactly
- Provider names match trait implementations
- Table names match migration SQL
- API endpoint paths match router registration

---

## Phase 4: Validation Report & Handoff

### Step 4.1 — Create validation report

**File to create:** `docs/roadmap/cold-start-watchtower-rag/validation-report.md`

**Structure:**

```markdown
# Validation Report — Cold-Start Watchtower RAG

## Epic Summary
[One paragraph summary of the full epic: Sessions 01–07]

## Quality Gate Results

| Check | Result | Notes |
|-------|--------|-------|
| `cargo fmt --all --check` | PASS/FAIL | |
| `RUSTFLAGS="-D warnings" cargo test --workspace` | PASS/FAIL | Test count: X |
| `cargo clippy --workspace -- -D warnings` | PASS/FAIL | |
| `cd dashboard && npm run check` | PASS/FAIL | Pre-existing warnings: N |

## Test Coverage Summary

| Component | Tests | Status |
|-----------|-------|--------|
| LocalFsProvider | N | PASS |
| GoogleDriveProvider (unit) | N | PASS |
| Ingest pipeline (shared) | N | PASS |
| Storage helpers | N | PASS |
| Config round-trip | N | PASS |
| Front-matter parsing | N | PASS |
| Seed worker | N | PASS |
| Winning DNA | N | PASS |
| E2E local→seed pipeline | N | PASS |
| E2E Drive→seed pipeline | N | PASS |
| Mixed sources | N | PASS |
| Loopback metadata | N | PASS |

## Manual Test Results

### Tauri Folder-Pick Flow
[Results from Step 2.6]

### Simulated Google Drive Sync
[Results from Step 2.7]

### POST /api/ingest
[Results from Step 2.3]

## Unresolved Risks

| ID | Risk | Severity | Mitigation |
|----|------|----------|------------|
| 1 | Google Drive JWT auth untested with real service account | Medium | Self-contained RSA signing works in unit tests; real key testing deferred to first user deployment. Rollback: disable Drive sources. |
| 2 | Dashboard manages only `sources[0]` | Low | Multiple sources work via config.toml editing. Dashboard multi-source UX is post-v1. |
| 3 | No retry/backoff for remote source errors | Low | Errors are logged and source status set to "error". Next poll retries. Exponential backoff is a follow-up. |
| 4 | Seed worker requires LLM availability | Low | If LLM is down, nodes stay 'pending' and retry on next tick. No data loss. |
| 5 | BigUint RSA implementation in google_drive.rs | Medium | Minimal, correct for PKCS#1 v1.5 signing. Tested with known test vectors. Consider replacing with `rsa` crate if issues arise in production. |

## Rollback Plan

If issues are discovered post-release:
1. **Disable Watchtower:** Remove `[[content_sources.sources]]` from config.toml. The watcher exits immediately with no sources configured.
2. **Remove Drive sources:** Change `source_type` back to `"local_fs"` or remove the source entry entirely.
3. **Data cleanup:** `DELETE FROM draft_seeds; DELETE FROM content_nodes; DELETE FROM source_contexts;` — these tables are additive and have no FK constraints from existing tables.
4. **Migration is additive:** The watchtower migration uses `CREATE TABLE IF NOT EXISTS` and `ALTER TABLE ADD COLUMN`. No destructive schema changes. Safe to leave in place.

## Follow-Up Work (post-release)

| Priority | Item | Effort |
|----------|------|--------|
| P1 | Test Google Drive auth with real service account key | 1 session |
| P2 | Dashboard multi-source management UI | 1-2 sessions |
| P2 | Exponential backoff for remote source errors | 0.5 sessions |
| P3 | Google Docs → Markdown export | 1 session |
| P3 | Interactive OAuth flow for personal Drive accounts | 1-2 sessions |
| P3 | Additional providers (Notion, Dropbox) | 1 session each |

## Recommendation

**[GO / NO-GO]**: [Decision and rationale]
```

### Step 4.2 — Create session-07 handoff

**File to create:** `docs/roadmap/cold-start-watchtower-rag/session-07-handoff.md`

**Structure:**

```markdown
# Session 07 Handoff — Cross-Source Validation & Release

## Summary
[What was done in this session]

## Quality Gates
[Table of results]

## New Tests Added
[List of integration tests from Phase 2]

## Documentation Updated
[List of doc changes from Phase 3]

## Validation Result
[Link to validation-report.md, go/no-go decision]

## Files Created
[List]

## Files Modified
[List]

## Next Steps
[Based on validation report — if GO: deployment guidance; if NO-GO: specific fixes needed]
```

---

## Files to Create

| File | Purpose |
|------|---------|
| `docs/roadmap/cold-start-watchtower-rag/validation-report.md` | Go/no-go release recommendation |
| `docs/roadmap/cold-start-watchtower-rag/session-07-handoff.md` | Session handoff for continuity |

## Files to Modify

| File | Changes |
|------|---------|
| `docs/architecture.md` | Add Content Source Pipeline section, update Key Modules and Runtime Loops tables |
| `docs/configuration.md` | Add `[content_sources]` documentation with both provider types |
| `crates/tuitbot-core/src/source/tests.rs` | Add E2E integration tests (Steps 2.1, 2.2, 2.4) |
| `crates/tuitbot-server/src/routes/ingest.rs` | Add inline node integration test (Step 2.3) — may require test infrastructure |
| `crates/tuitbot-core/src/automation/watchtower/tests.rs` | Add loopback integration test (Step 2.5) if not already covered |

## Key Design Decisions

1. **No new business logic in this session.** This is purely validation, testing, and documentation. The provider model, storage layer, and pipeline are feature-complete from Sessions 02-06.

2. **Integration tests use mock LLM.** The seed worker tests use a `MockLlm` that returns deterministic HOOK/FORMAT responses. This avoids external service dependencies while still validating the full chain.

3. **Google Drive validation is simulated.** Without a real service account key, we validate the Drive code path through `ingest_content()` with `gdrive://` provider IDs. The JWT auth and API calls are tested in isolation.

4. **Documentation follows existing patterns.** The architecture.md and configuration.md updates use the same formatting, table structures, and detail level as the existing content.

5. **Rollback is clean.** All Watchtower changes are additive (new tables, new fields with `serde(default)`, new modules). Removing the `[[content_sources.sources]]` config section disables the feature entirely with zero impact on existing functionality.

## Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| New integration tests increase test suite time | Low | Tests use `init_test_db()` (in-memory SQLite) and mock LLM — millisecond execution |
| Doc updates may drift from code | Low | Cross-reference step (3.3) catches mismatches before commit |
| Test file `source/tests.rs` may exceed 500-line limit | Medium | If so, split into `source/tests/unit.rs` and `source/tests/integration.rs` following the module directory pattern |
| Server integration test may need test infrastructure | Medium | If `tuitbot-server` lacks in-process test helpers, use the inline node test via direct function call rather than HTTP |

## Verification Steps

After completing all phases, run the full quality gate suite one final time:

```bash
cargo fmt --all && cargo fmt --all --check
RUSTFLAGS="-D warnings" cargo test --workspace
cargo clippy --workspace -- -D warnings
cd dashboard && npm run check
```

All four must pass before writing the final validation report and handoff.

## Exact Order of Operations

1. **Phase 1.1** — `cargo fmt --all && cargo fmt --all --check` → fix any formatting issues
2. **Phase 1.2** — `RUSTFLAGS="-D warnings" cargo test --workspace` → fix any test failures
3. **Phase 1.3** — `cargo clippy --workspace -- -D warnings` → fix any clippy warnings
4. **Phase 1.4** — `cd dashboard && npm run check` → fix any dashboard warnings
5. **Phase 1.5** — Review existing test coverage (read test files, no code changes)
6. **Phase 2.1** — Write E2E test: local folder → seed pipeline
7. **Phase 2.2** — Write E2E test: simulated Google Drive → seed pipeline
8. **Phase 2.3** — Write E2E test: POST /api/ingest inline nodes
9. **Phase 2.4** — Write E2E test: mixed sources → draft context
10. **Phase 2.5** — Write integration test: loopback metadata
11. **Phase 2.6** — Document manual Tauri test procedure
12. **Phase 2.7** — Document manual Drive simulation procedure
13. **Run quality gates again** — all must pass with new tests
14. **Phase 3.1** — Update `docs/architecture.md`
15. **Phase 3.2** — Update `docs/configuration.md`
16. **Phase 3.3** — Cross-reference docs against code
17. **Phase 4.1** — Write `validation-report.md`
18. **Phase 4.2** — Write `session-07-handoff.md`
19. **Final quality gate run** — verify everything still passes
20. **Commit** — stage all changes, commit with descriptive message
