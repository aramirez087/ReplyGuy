# Session 02 — Implementation Plan: Schema and Ingest API

## Objective

Implement the Watchtower ingestion foundation: schema migration, storage CRUD, configuration extension, and POST /api/ingest HTTP contract. No source trait or filesystem scanning in this session (deferred to Session 03 per deliverables list).

---

## Scope Reconciliation

The session instructions' deliverables list is the authority. The S01 handoff proposed also creating `source/mod.rs` and `source/local_fs.rs` in Session 02, but these are **not** in the session deliverables. This plan limits scope to exactly:

| Deliverable | Type |
|------------|------|
| `migrations/20260228000019_watchtower_ingestion.sql` | Create |
| `crates/tuitbot-core/src/storage/watchtower.rs` | Create |
| `crates/tuitbot-core/src/storage/mod.rs` | Modify |
| `crates/tuitbot-core/src/config/types.rs` | Modify |
| `crates/tuitbot-core/src/config/mod.rs` | Modify (re-export + field) |
| `crates/tuitbot-server/src/routes/ingest.rs` | Create |
| `crates/tuitbot-server/src/routes/mod.rs` | Modify |
| `crates/tuitbot-server/src/lib.rs` | Modify |
| `crates/tuitbot-server/tests/api_tests.rs` | Modify |
| `docs/roadmap/cold-start-watchtower-rag/session-02-handoff.md` | Create |

No new crate dependencies are needed — `sha2` is already in `tuitbot-core/Cargo.toml` (line 28). `serde_yaml` is deferred to Session 03 when front-matter parsing is implemented.

---

## Key Design Decisions

### D1: Ingest route delegates to storage CRUD directly (no source trait yet)

Since the `ContentSource` trait and `LocalFileSource` are not in scope, POST /api/ingest will perform a thin storage-only operation: register/retrieve a source context and upsert content nodes from the request payload. The actual filesystem scanning and content reading will be wired in Session 03 when the source trait and Watchtower loop land.

**Rationale:** Ship the HTTP contract and storage layer first so they can be tested independently. The ingest handler's signature and response shape are stable — only the internal implementation grows in Session 03.

### D2: Ingest handler accepts inline content for testing, hints for future

The ingest request will support two modes:
- **`file_hints`** (Vec<String>): Relative paths to re-scan. In Session 02, this returns a placeholder acknowledging the hints but not scanning (no source trait yet). Full scanning lands in S03.
- **`inline_nodes`** (Vec<InlineNode>): Direct content submission for testing and for Shortcuts/Telegram triggers that already have the content. This is immediately operational.

This means the ingest route is functional in S02 (via inline_nodes) while file_hints gain real behavior in S03.

### D3: Storage watchtower CRUD follows analytics.rs patterns exactly

Studied `storage/analytics.rs`:
- Free functions taking `&DbPool` as first arg
- `StorageError` for all errors via `map_err(|e| StorageError::Query { source: e })?`
- Return typed structs, not raw tuples
- `DEFAULT_ACCOUNT_ID` from `accounts` module
- `init_test_db()` for test setup

### D4: Config addition is purely additive with `#[serde(default)]`

New `ContentSourcesConfig` added to `Config` struct. Existing configs without `[content_sources]` deserialize identically. Verified by reading `config/mod.rs:67-135` — every field has `#[serde(default)]`.

---

## Risks and Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| `ALTER TABLE ADD COLUMN` on tables with existing data | Low — SQLite ADD COLUMN is safe for nullable columns with no default or with `DEFAULT` | New columns are nullable (`TEXT` / `REAL` without `NOT NULL`), safe for existing rows |
| Migration order conflict if another branch adds migration 19 | Medium — migration name collision | Migration name `20260228000019_watchtower_ingestion.sql` is locked per handoff. Only one branch should be active. |
| Ingest route without auth returns data to unauthenticated users | High | Route is inside the `api` router which has auth middleware applied. Not in `AUTH_EXEMPT_PATHS`. |
| New `content_sources` config field breaks existing TOML parsing | Low | `#[serde(default)]` on the field + `ContentSourcesConfig::default()` returns empty sources list. Verified by test. |
| Storage tests exceed 500-line file limit | Medium | If `watchtower.rs` approaches limit, split tests into `storage/watchtower/tests.rs` submodule. Plan for ~300 lines of CRUD + ~200 lines of tests. If over limit, use module directory pattern. |

---

## Order of Operations

### Step 1: Migration SQL

**File:** `migrations/20260228000019_watchtower_ingestion.sql`

Create the migration with exact SQL from AD-2:

```sql
-- Watchtower ingestion tables

-- Registered content sources
CREATE TABLE IF NOT EXISTS source_contexts (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    account_id TEXT NOT NULL DEFAULT '00000000-0000-0000-0000-000000000000',
    source_type TEXT NOT NULL,
    config_json TEXT NOT NULL DEFAULT '{}',
    sync_cursor TEXT,
    status TEXT NOT NULL DEFAULT 'active',
    error_message TEXT,
    created_at TEXT NOT NULL DEFAULT (datetime('now')),
    updated_at TEXT NOT NULL DEFAULT (datetime('now'))
);

-- Ingested content chunks from sources
CREATE TABLE IF NOT EXISTS content_nodes (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    account_id TEXT NOT NULL DEFAULT '00000000-0000-0000-0000-000000000000',
    source_id INTEGER NOT NULL REFERENCES source_contexts(id),
    relative_path TEXT NOT NULL,
    content_hash TEXT NOT NULL,
    title TEXT,
    body_text TEXT NOT NULL,
    front_matter_json TEXT,
    tags TEXT,
    status TEXT NOT NULL DEFAULT 'pending',
    ingested_at TEXT NOT NULL DEFAULT (datetime('now')),
    updated_at TEXT NOT NULL DEFAULT (datetime('now')),
    UNIQUE(source_id, relative_path)
);

-- Pre-computed draft seeds
CREATE TABLE IF NOT EXISTS draft_seeds (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    account_id TEXT NOT NULL DEFAULT '00000000-0000-0000-0000-000000000000',
    node_id INTEGER NOT NULL REFERENCES content_nodes(id),
    seed_text TEXT NOT NULL,
    archetype_suggestion TEXT,
    engagement_weight REAL NOT NULL DEFAULT 0.5,
    status TEXT NOT NULL DEFAULT 'pending',
    created_at TEXT NOT NULL DEFAULT (datetime('now')),
    used_at TEXT
);

-- Indexes for new tables
CREATE INDEX IF NOT EXISTS idx_content_nodes_source ON content_nodes(source_id, status);
CREATE INDEX IF NOT EXISTS idx_content_nodes_hash ON content_nodes(content_hash);
CREATE INDEX IF NOT EXISTS idx_draft_seeds_status ON draft_seeds(status, engagement_weight DESC);
CREATE INDEX IF NOT EXISTS idx_draft_seeds_node ON draft_seeds(node_id);

-- Additive columns on existing tables
ALTER TABLE tweet_performance ADD COLUMN archetype_vibe TEXT;
ALTER TABLE reply_performance ADD COLUMN archetype_vibe TEXT;
ALTER TABLE tweet_performance ADD COLUMN engagement_score REAL;
ALTER TABLE reply_performance ADD COLUMN engagement_score REAL;
ALTER TABLE original_tweets ADD COLUMN source_node_id INTEGER REFERENCES content_nodes(id);
```

**Verification:** `init_test_db()` runs all migrations on a fresh in-memory DB. The test suite confirms these tables exist.

**Important:** All `ALTER TABLE ADD COLUMN` statements add nullable columns (no `NOT NULL`), which is safe for existing rows in SQLite.

### Step 2: Config Types Extension

**File:** `crates/tuitbot-core/src/config/types.rs`

Add at the end (before existing default value functions or in a new section):

```rust
// ---------------------------------------------------------------------------
// Content Sources
// ---------------------------------------------------------------------------

/// Content source configuration for the Watchtower.
#[derive(Debug, Clone, Default, Deserialize, Serialize)]
pub struct ContentSourcesConfig {
    /// Configured content sources.
    #[serde(default)]
    pub sources: Vec<ContentSourceEntry>,
}

/// A single content source entry.
#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct ContentSourceEntry {
    /// Source type: "local_fs" (v1). Future: "google_drive".
    #[serde(default = "default_source_type")]
    pub source_type: String,

    /// Filesystem path (for local_fs sources). Supports ~ expansion.
    #[serde(default)]
    pub path: Option<String>,

    /// Whether to watch for changes in real-time.
    #[serde(default = "default_watch")]
    pub watch: bool,

    /// File patterns to include.
    #[serde(default = "default_file_patterns")]
    pub file_patterns: Vec<String>,

    /// Whether to write metadata back to source files.
    #[serde(default = "default_loop_back")]
    pub loop_back_enabled: bool,
}

fn default_source_type() -> String {
    "local_fs".to_string()
}
fn default_watch() -> bool {
    true
}
fn default_file_patterns() -> Vec<String> {
    vec!["*.md".to_string(), "*.txt".to_string()]
}
fn default_loop_back() -> bool {
    true
}
```

**File:** `crates/tuitbot-core/src/config/mod.rs`

1. Add to the `pub use types::` line (line 21-24):
   ```rust
   pub use types::{
       AuthConfig, BusinessProfile, ContentSourceEntry, ContentSourcesConfig,
       IntervalsConfig, LimitsConfig, LlmConfig, LoggingConfig,
       ScoringConfig, ServerConfig, StorageConfig, TargetsConfig, XApiConfig,
   };
   ```

2. Add field to `Config` struct (after `circuit_breaker`, line ~134):
   ```rust
   /// Content source configuration for the Watchtower.
   #[serde(default)]
   pub content_sources: ContentSourcesConfig,
   ```

### Step 3: Storage Watchtower Module

**File:** `crates/tuitbot-core/src/storage/watchtower.rs` (new)

Create typed CRUD helpers following the `analytics.rs` pattern:

**Structs:**
- `SourceContext` — row struct for `source_contexts` table
- `ContentNode` — row struct for `content_nodes` table
- `DraftSeed` — row struct for `draft_seeds` table
- `UpsertResult` — enum { Inserted, Updated, Skipped } for dedup feedback

**Functions (all `pub async fn`):**

| Function | SQL Pattern | Notes |
|----------|------------|-------|
| `insert_source_context(pool, source_type, config_json) -> Result<i64>` | INSERT RETURNING id | Returns new row ID |
| `get_source_context(pool, id) -> Result<Option<SourceContext>>` | SELECT by id | |
| `get_source_contexts(pool) -> Result<Vec<SourceContext>>` | SELECT all active | |
| `update_sync_cursor(pool, id, cursor) -> Result<()>` | UPDATE sync_cursor, updated_at | |
| `update_source_status(pool, id, status, error_message) -> Result<()>` | UPDATE status, error_message | |
| `upsert_content_node(pool, source_id, relative_path, content_hash, title, body_text, front_matter_json, tags) -> Result<UpsertResult>` | INSERT ON CONFLICT(source_id, relative_path) | Compare hash: same hash → Skipped, different hash → Updated |
| `get_content_node(pool, id) -> Result<Option<ContentNode>>` | SELECT by id | |
| `get_nodes_for_source(pool, source_id, status) -> Result<Vec<ContentNode>>` | SELECT by source_id and optional status filter | |
| `insert_draft_seed(pool, node_id, seed_text, archetype_suggestion) -> Result<i64>` | INSERT RETURNING id | |
| `get_pending_seeds(pool, limit) -> Result<Vec<DraftSeed>>` | SELECT WHERE status='pending' ORDER BY engagement_weight DESC | |
| `mark_seed_used(pool, id) -> Result<()>` | UPDATE status='used', used_at=datetime('now') | |

**Key implementation details:**
- Use `DEFAULT_ACCOUNT_ID` from `storage::accounts` for all inserts
- Upsert logic: `INSERT OR REPLACE` won't work (loses id). Use `INSERT ... ON CONFLICT(source_id, relative_path) DO UPDATE SET` with conditional hash check
- Actually, the upsert should: INSERT → on conflict check hash → if hash differs, update body/title/tags/hash/updated_at → return Updated. If hash same → return Skipped. Can be done with a single SQL: `INSERT ... ON CONFLICT DO UPDATE SET ... WHERE content_nodes.content_hash != excluded.content_hash` then check `changes()`.
- Return `UpsertResult` based on `changes()` count from SQLite

**Tests (inline `#[cfg(test)]` module):**

Per test strategy, implement:
1. `migration_creates_new_tables` — query `sqlite_master`
2. `migration_adds_columns_to_performance` — `PRAGMA table_info`
3. `migration_adds_source_node_id_to_tweets` — `PRAGMA table_info`
4. `insert_and_get_source_context`
5. `update_sync_cursor`
6. `insert_content_node`
7. `content_node_upsert_by_hash` — different hash → Updated
8. `content_node_dedup_same_hash` — same hash → Skipped
9. `insert_draft_seed`
10. `get_pending_seeds_ordered_by_weight`
11. `mark_seed_used`
12. `get_nodes_for_source`

**File size estimate:** ~280 lines CRUD + ~220 lines tests = ~500 lines. If approaching limit, split into `storage/watchtower/mod.rs` + `storage/watchtower/tests.rs`.

**Decision: Use module directory pattern from the start.** Given ~500 lines estimated, create:
- `crates/tuitbot-core/src/storage/watchtower/mod.rs` — CRUD functions + structs (~280 lines)
- `crates/tuitbot-core/src/storage/watchtower/tests.rs` — all tests (~220 lines)

This follows the project's established pattern (e.g., `commands/settings/`).

**File:** `crates/tuitbot-core/src/storage/mod.rs`

Add `pub mod watchtower;` after the existing module declarations (after line 26, `pub mod x_api_usage;`).

### Step 4: Ingest Route

**File:** `crates/tuitbot-server/src/routes/ingest.rs` (new)

Follow `settings.rs` pattern: extract `State<Arc<AppState>>`, parse JSON body, delegate to core storage, return JSON response.

**Request/Response types:**

```rust
#[derive(Deserialize)]
pub struct IngestRequest {
    /// Source type to target. Default: first configured source.
    #[serde(default)]
    pub source_type: Option<String>,
    /// Specific files to re-scan (relative paths within source).
    #[serde(default)]
    pub file_hints: Vec<String>,
    /// Re-ingest even if content hash unchanged.
    #[serde(default)]
    pub force: bool,
    /// Inline content nodes for direct ingestion (Shortcuts/Telegram).
    #[serde(default)]
    pub inline_nodes: Vec<InlineNode>,
}

#[derive(Deserialize)]
pub struct InlineNode {
    pub relative_path: String,
    pub body_text: String,
    #[serde(default)]
    pub title: Option<String>,
    #[serde(default)]
    pub tags: Option<String>,
}

#[derive(Serialize)]
pub struct IngestResponse {
    pub ingested: u32,
    pub skipped: u32,
    pub errors: Vec<String>,
    pub duration_ms: u64,
}
```

**Handler logic (`pub async fn ingest`):**
1. Start timer (`Instant::now()`)
2. If `inline_nodes` is non-empty: for each node, compute SHA-256 hash of body_text, call `storage::watchtower::upsert_content_node`. Track ingested/skipped/errors.
3. If `file_hints` is non-empty but no inline content: return response noting file scanning not yet implemented (S02 placeholder). In S03, this delegates to the ContentSource trait.
4. If both empty: return response with zeros (no-op).
5. Return `IngestResponse` with counts and elapsed time.

**SHA-256 hashing:** Use the existing `sha2` crate (`use sha2::{Sha256, Digest}`) — already in `tuitbot-core/Cargo.toml`.

**Source context management:** For inline ingestion, ensure a "manual" source context exists (create one if needed with `source_type = "manual"`). Cache the source_id across the request.

**File:** `crates/tuitbot-server/src/routes/mod.rs`

Add `pub mod ingest;` after existing module declarations.

**File:** `crates/tuitbot-server/src/lib.rs`

Add `.route("/ingest", post(routes::ingest::ingest))` in the API router chain. Place it near the content/drafts routes (around line 101) for logical grouping. The route sits inside the `let api = Router::new()` block which has auth middleware applied at the end (line 227-230), so it's automatically authenticated.

### Step 5: Integration Tests

**File:** `crates/tuitbot-server/tests/api_tests.rs`

Add test functions at the end of the file, using existing helpers (`test_router`, `post_json`):

```rust
// ============================================================
// Ingest
// ============================================================

#[tokio::test]
async fn post_ingest_returns_200() {
    let router = test_router().await;
    let (status, body) = post_json(
        router,
        "/api/ingest",
        serde_json::json!({
            "inline_nodes": [{
                "relative_path": "notes/test.md",
                "body_text": "Some test content about Rust.",
                "title": "Test Note"
            }]
        }),
    )
    .await;
    assert_eq!(status, StatusCode::OK);
    assert_eq!(body["ingested"], 1);
    assert_eq!(body["skipped"], 0);
    assert!(body["duration_ms"].is_number());
}

#[tokio::test]
async fn post_ingest_requires_auth() {
    let router = test_router().await;
    let req = Request::builder()
        .method("POST")
        .uri("/api/ingest")
        .header("Content-Type", "application/json")
        .body(Body::from(
            serde_json::to_vec(&serde_json::json!({"inline_nodes": []})).unwrap(),
        ))
        .expect("build request");
    let response = router.oneshot(req).await.expect("send request");
    assert_eq!(response.status(), StatusCode::UNAUTHORIZED);
}

#[tokio::test]
async fn post_ingest_idempotent() {
    // Uses inline state construction (not test_router) to share pool
    let pool = storage::init_test_db().await.expect("init test db");
    let (event_tx, _) = tokio::sync::broadcast::channel::<WsEvent>(256);
    let state = Arc::new(AppState { /* ... same fields as existing tests ... */ });
    let router = tuitbot_server::build_router(state);

    let body = serde_json::json!({
        "inline_nodes": [{
            "relative_path": "notes/idea.md",
            "body_text": "Content that won't change."
        }]
    });

    // First ingest
    let (status, resp1) = post_json(router.clone(), "/api/ingest", body.clone()).await;
    assert_eq!(status, StatusCode::OK);
    assert_eq!(resp1["ingested"], 1);

    // Second ingest — same content, same hash → skipped
    let (status, resp2) = post_json(router, "/api/ingest", body).await;
    assert_eq!(status, StatusCode::OK);
    assert_eq!(resp2["ingested"], 0);
    assert_eq!(resp2["skipped"], 1);
}

#[tokio::test]
async fn post_ingest_empty_body() {
    let router = test_router().await;
    let (status, body) = post_json(router, "/api/ingest", serde_json::json!({})).await;
    assert_eq!(status, StatusCode::OK);
    assert_eq!(body["ingested"], 0);
    assert_eq!(body["skipped"], 0);
}
```

### Step 6: Config Tests

**File:** `crates/tuitbot-core/src/config/tests.rs`

Add at the end:

```rust
#[test]
fn content_sources_config_serde_roundtrip() {
    let toml_str = r#"
[[content_sources.sources]]
source_type = "local_fs"
path = "~/notes/vault"
watch = true
file_patterns = ["*.md", "*.txt"]
loop_back_enabled = true
"#;
    let config: Config = toml::from_str(toml_str).expect("valid TOML");
    assert_eq!(config.content_sources.sources.len(), 1);
    let source = &config.content_sources.sources[0];
    assert_eq!(source.source_type, "local_fs");
    assert_eq!(source.path.as_deref(), Some("~/notes/vault"));
    assert!(source.watch);
    assert_eq!(source.file_patterns, vec!["*.md", "*.txt"]);
    assert!(source.loop_back_enabled);
}

#[test]
fn content_sources_defaults() {
    let toml_str = r#"
[[content_sources.sources]]
path = "~/notes"
"#;
    let config: Config = toml::from_str(toml_str).expect("valid TOML");
    let source = &config.content_sources.sources[0];
    assert_eq!(source.source_type, "local_fs");
    assert!(source.watch);
    assert_eq!(source.file_patterns, vec!["*.md", "*.txt"]);
    assert!(source.loop_back_enabled);
}

#[test]
fn content_sources_optional_in_config() {
    // A config without [content_sources] should parse fine
    let toml_str = r#"
[business]
product_name = "Test"
"#;
    let config: Config = toml::from_str(toml_str).expect("valid TOML");
    assert!(config.content_sources.sources.is_empty());
}
```

### Step 7: Update init_test_db table assertion

**File:** `crates/tuitbot-core/src/storage/mod.rs`

Add assertions for the new tables in the `init_test_db_creates_all_tables` test (around line 150-173):

```rust
assert!(table_names.contains(&"source_contexts"));
assert!(table_names.contains(&"content_nodes"));
assert!(table_names.contains(&"draft_seeds"));
```

### Step 8: Handoff Document

**File:** `docs/roadmap/cold-start-watchtower-rag/session-02-handoff.md`

Document:
- What was completed (migration, storage CRUD, config, ingest route, tests)
- Decisions made in this session
- Files created and modified
- What Session 03 must do (ContentSource trait, LocalFileSource, Watchtower loop, wire file_hints into real scanning)
- Any deferred items

---

## Exact File Edit Sequence

Execute in this order to minimize broken intermediate states:

1. **Create** `migrations/20260228000019_watchtower_ingestion.sql`
   - No dependencies, standalone

2. **Modify** `crates/tuitbot-core/src/config/types.rs`
   - Add `ContentSourcesConfig`, `ContentSourceEntry` structs + default fns

3. **Modify** `crates/tuitbot-core/src/config/mod.rs`
   - Add `ContentSourcesConfig, ContentSourceEntry` to re-export
   - Add `pub content_sources: ContentSourcesConfig` field to `Config`

4. **Create** `crates/tuitbot-core/src/storage/watchtower/mod.rs`
   - All CRUD functions + structs + UpsertResult enum

5. **Create** `crates/tuitbot-core/src/storage/watchtower/tests.rs`
   - All 12 storage tests

6. **Modify** `crates/tuitbot-core/src/storage/mod.rs`
   - Add `pub mod watchtower;`
   - Add new table assertions to existing test

7. **Run** `cargo test -p tuitbot-core` to verify storage + config tests pass

8. **Create** `crates/tuitbot-server/src/routes/ingest.rs`
   - Request/response types, handler function

9. **Modify** `crates/tuitbot-server/src/routes/mod.rs`
   - Add `pub mod ingest;`

10. **Modify** `crates/tuitbot-server/src/lib.rs`
    - Add `.route("/ingest", post(routes::ingest::ingest))`

11. **Modify** `crates/tuitbot-server/tests/api_tests.rs`
    - Add ingest route tests

12. **Modify** `crates/tuitbot-core/src/config/tests.rs`
    - Add content_sources config tests

13. **Run quality gates:**
    ```bash
    cargo fmt --all && cargo fmt --all --check
    RUSTFLAGS="-D warnings" cargo test --workspace
    cargo clippy --workspace -- -D warnings
    ```

14. **Create** `docs/roadmap/cold-start-watchtower-rag/session-02-handoff.md`

---

## Verification Steps

### Automated (CI Checklist)

```bash
cargo fmt --all && cargo fmt --all --check
RUSTFLAGS="-D warnings" cargo test --workspace
cargo clippy --workspace -- -D warnings
```

### Manual Spot Checks

1. **Migration applies cleanly:** `init_test_db()` in any test → no migration errors
2. **Tables exist:** `migration_creates_new_tables` test passes
3. **ALTER columns exist:** `migration_adds_columns_to_performance` test passes
4. **Config backward-compat:** `content_sources_optional_in_config` test passes
5. **Ingest authenticated:** `post_ingest_requires_auth` returns 401
6. **Ingest functional:** `post_ingest_returns_200` returns 200 with ingested=1
7. **Ingest idempotent:** `post_ingest_idempotent` returns skipped=1 on second call
8. **No warnings:** `RUSTFLAGS="-D warnings"` flag catches any compiler warnings
9. **File size limits:** No .rs file exceeds 500 lines

### Exit Criteria Verification

| Criterion | How to verify |
|-----------|--------------|
| Migration applies cleanly on fresh test database | `init_test_db()` runs in every storage test |
| POST /api/ingest registered in router | `post_ingest_returns_200` test |
| POST /api/ingest has automated contract coverage | 4 tests in api_tests.rs |
| Config shape can express local Obsidian source | `content_sources_config_serde_roundtrip` test |
| Config doesn't break existing configs | `content_sources_optional_in_config` test |

---

## Appendix: Codebase Patterns to Follow

| Pattern | Source | Applied in |
|---------|--------|-----------|
| Storage function signature | `analytics.rs:25` — `pub async fn fn_name(pool: &DbPool, ...) -> Result<T, StorageError>` | All watchtower CRUD |
| Error mapping | `analytics.rs:47` — `.map_err(\|e\| StorageError::Query { source: e })?` | All SQL queries |
| Test DB setup | `analytics.rs:653` — `let pool = init_test_db().await.expect("init db");` | All storage tests |
| Config struct | `types.rs:10` — `#[derive(Debug, Clone, Default, Deserialize, Serialize)]` | ContentSourcesConfig |
| Config field | `mod.rs:73` — `#[serde(default)]` on every Config field | content_sources field |
| Route handler | `settings.rs:110` — `pub async fn fn_name(State(state): State<Arc<AppState>>) -> Json<Value>` | ingest handler |
| API test | `api_tests.rs:123` — `let router = test_router().await;` + assert on status/body | ingest tests |
| Module file limit | CLAUDE.md — 500 lines max, split into directory | watchtower/ directory |
| DEFAULT_ACCOUNT_ID | `accounts.rs:10` — `"00000000-0000-0000-0000-000000000000"` | All INSERT statements |
